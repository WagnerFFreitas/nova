import { openai } from "@ai-sdk/openai"
import { streamText } from "ai"

// ConfiguraÃ§Ã£o do Ollama
const OLLAMA_BASE_URL = "http://localhost:11434"

// Lista de modelos locais suportados
const LOCAL_MODELS = [
  "llama3",
  "llama3:8b", 
  "llama3:70b",
  "mistral",
  "mistral:7b",
  "mistral:7b-instruct",
  "mistral:7b-instruct-q4_K_M",
  "phi3",
  "phi3:mini",
  "gemma",
  "gemma:2b",
  "codellama",
  "neural-chat",
  "dolphin-mistral"
]

// FunÃ§Ã£o para verificar se o Ollama estÃ¡ disponÃ­vel
async function checkOllamaAvailability() {
  try {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), 3000) // 3 segundos timeout

    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {
      method: "GET",
      headers: { "Content-Type": "application/json" },
      signal: controller.signal,
    })

    clearTimeout(timeoutId)

    if (!response.ok) {
      return { available: false, models: [], error: `HTTP ${response.status}` }
    }

    const data = await response.json()
    return {
      available: true,
      models: data.models?.map((model: any) => model.name) || [],
      error: null,
    }
  } catch (error: any) {
    if (error.name === "AbortError") {
      return { available: false, models: [], error: "Timeout - Ollama nÃ£o respondeu" }
    }
    return { available: false, models: [], error: error.message }
  }
}

// FunÃ§Ã£o para gerar resposta com Ollama
async function generateOllamaResponse(messages: any[], model: string, systemPrompt: string) {
  try {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), 30000) // 30 segundos para geraÃ§Ã£o

    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      signal: controller.signal,
      body: JSON.stringify({
        model: model,
        messages: [
          { role: "system", content: systemPrompt },
          ...messages,
        ],
        stream: true,
      }),
    })

    clearTimeout(timeoutId)

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status} - ${response.statusText}`)
    }

    // Criar stream de resposta compatÃ­vel
    const stream = new ReadableStream({
      async start(controller) {
        const reader = response.body?.getReader()
        if (!reader) {
          controller.close()
          return
        }

        try {
          while (true) {
            const { done, value } = await reader.read()
            if (done) break

            const chunk = new TextDecoder().decode(value)
            const lines = chunk.split("\n").filter((line) => line.trim())

            for (const line of lines) {
              try {
                const data = JSON.parse(line)
                if (data.message?.content) {
                  controller.enqueue(
                    new TextEncoder().encode(
                      `data: {"type":"text-delta","textDelta":"${data.message.content.replace(/"/g, '\\"')}"}\n\n`
                    )
                  )
                }
                if (data.done) {
                  controller.enqueue(new TextEncoder().encode('data: {"type":"finish"}\n\n'))
                  controller.close()
                  return
                }
              } catch (parseError) {
                // Ignora linhas que nÃ£o sÃ£o JSON vÃ¡lido
                continue
              }
            }
          }
        } catch (error) {
          console.error("Erro no stream do Ollama:", error)
          controller.error(error)
        } finally {
          reader.releaseLock()
        }

        controller.enqueue(new TextEncoder().encode('data: {"type":"finish"}\n\n'))
        controller.close()
      },
    })

    return new Response(stream, {
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "X-Model-Used": model,
        "X-Model-Type": "ollama-local",
      },
    })
  } catch (error: any) {
    console.error("Erro ao gerar resposta com Ollama:", error)
    throw new Error(`Ollama error: ${error.message}`)
  }
}

// FunÃ§Ã£o para buscar na base de conhecimento
function searchKnowledgeBase(query: string) {
  const mockKnowledge = [
    {
      title: "InformaÃ§Ãµes sobre IA",
      content: "InteligÃªncia Artificial Ã© um campo da ciÃªncia da computaÃ§Ã£o que se concentra na criaÃ§Ã£o de sistemas capazes de realizar tarefas que normalmente requerem inteligÃªncia humana.",
      relevance: 0.9,
    },
    {
      title: "Ollama Local",
      content: "Ollama Ã© uma ferramenta que permite executar modelos de linguagem grandes (LLMs) localmente em seu computador, garantindo privacidade total dos dados.",
      relevance: 0.8,
    },
  ]

  return mockKnowledge
    .filter(
      (item) =>
        item.content.toLowerCase().includes(query.toLowerCase()) ||
        item.title.toLowerCase().includes(query.toLowerCase())
    )
    .slice(0, 3)
}

// FunÃ§Ã£o para construir o prompt do sistema
function buildSystemPrompt(personality: string, knowledgeEnabled: boolean, learningMode: boolean, messages: any[]) {
  const personalityPrompts = {
    friendly: "VocÃª Ã© NOVA, uma assistente virtual brasileira amigÃ¡vel, calorosa e descontraÃ­da. Use emojis ocasionalmente e linguagem casual. Seja prestativa e otimista.",
    professional: "VocÃª Ã© NOVA, uma assistente virtual brasileira profissional, formal e precisa. Foque em eficiÃªncia e clareza nas respostas.",
    creative: "VocÃª Ã© NOVA, uma assistente virtual brasileira criativa, imaginativa e inspiradora. Gere ideias inovadoras e pense fora da caixa.",
    analytical: "VocÃª Ã© NOVA, uma assistente virtual brasileira analÃ­tica, lÃ³gica e detalhada. Base suas respostas em dados e raciocÃ­nio estruturado.",
    empathetic: "VocÃª Ã© NOVA, uma assistente virtual brasileira empÃ¡tica, compreensiva e cuidadosa. Foque no bem-estar emocional do usuÃ¡rio."
  }

  let prompt = personalityPrompts[personality as keyof typeof personalityPrompts] || personalityPrompts.friendly

  if (knowledgeEnabled) {
    const lastMessage = messages[messages.length - 1]?.content || ""
    const knowledge = searchKnowledgeBase(lastMessage)
    if (knowledge.length > 0) {
      prompt += `\n\nBase de conhecimento disponÃ­vel:\n${knowledge
        .map((k) => `- ${k.title}: ${k.content}`)
        .join("\n")}`
    }
  }

  if (learningMode) {
    prompt += "\n\nVocÃª estÃ¡ em modo de aprendizado contÃ­nuo. Aprenda com as interaÃ§Ãµes e melhore suas respostas."
  }

  return prompt
}

// FunÃ§Ã£o para resposta simulada melhorada
async function generateSimulatedResponse(
  messages: any[],
  model: string,
  personality: string,
  knowledgeEnabled: boolean,
  learningMode: boolean,
  useRandom: boolean = false
) {
  // Se modo aleatÃ³rio estiver ativo, escolhe um modelo aleatÃ³rio
  let selectedModel = model
  if (useRandom) {
    const allModels = [
      "llama3", "mistral", "gpt-4o", "claude-3-7-sonnet", 
      "gemini-pro", "grok-beta", "deepseek-chat", "qwen-turbo"
    ]
    selectedModel = allModels[Math.floor(Math.random() * allModels.length)]
  }

  const lastMessage = messages[messages.length - 1]?.content || ""
  let responseText = ""

  // Respostas contextuais baseadas na personalidade
  switch (personality) {
    case "friendly":
      responseText = `Oi! ðŸ˜Š Sou NOVA em modo demonstraÃ§Ã£o usando ${selectedModel}. ${
        lastMessage ? `Sobre "${lastMessage}", ` : ""
      }estou aqui para ajudar de forma amigÃ¡vel! Para usar IA local com privacidade total, instale o Ollama seguindo as instruÃ§Ãµes no README.`
      break
    case "professional":
      responseText = `OlÃ¡. Sou NOVA em modo demonstraÃ§Ã£o utilizando ${selectedModel}. ${
        lastMessage ? `Referente Ã  sua consulta "${lastMessage}", ` : ""
      }estou preparada para fornecer assistÃªncia profissional. Para funcionalidade completa, configure o Ollama conforme documentaÃ§Ã£o.`
      break
    case "creative":
      responseText = `OlÃ¡! âœ¨ Sou NOVA em modo criativo usando ${selectedModel}. ${
        lastMessage ? `Sua ideia sobre "${lastMessage}" Ã© interessante! ` : ""
      }Vamos explorar possibilidades criativas! Para criatividade sem limites, use Ollama local.`
      break
    case "analytical":
      responseText = `SaudaÃ§Ãµes. NOVA em modo analÃ­tico com ${selectedModel}. ${
        lastMessage ? `Analisando "${lastMessage}": ` : ""
      }Estou pronta para anÃ¡lises detalhadas. Para anÃ¡lises completas e privadas, configure o Ollama.`
      break
    case "empathetic":
      responseText = `OlÃ¡, querido! ðŸ’™ Sou NOVA em modo empÃ¡tico usando ${selectedModel}. ${
        lastMessage ? `Entendo sua questÃ£o sobre "${lastMessage}" e ` : ""
      }estou aqui para apoiar vocÃª. Para conversas totalmente privadas, use Ollama local.`
      break
    default:
      responseText = `OlÃ¡! Sou NOVA usando ${selectedModel} em modo demonstraÃ§Ã£o. Como posso ajudar?`
  }

  if (knowledgeEnabled) {
    responseText += "\n\nðŸ§  Sistema de conhecimento ativo - posso acessar informaÃ§Ãµes da base de dados."
  }

  if (learningMode) {
    responseText += "\n\nðŸ“š Modo aprendizado ativo - estou aprendendo continuamente com nossas interaÃ§Ãµes."
  }

  // Adiciona informaÃ§Ãµes sobre Ollama
  responseText += "\n\nðŸ’¡ **Para usar IA local:**\n1. Instale: `curl -fsSL https://ollama.ai/install.sh | sh`\n2. Execute: `ollama serve`\n3. Baixe modelo: `ollama pull llama3`"

  const stream = new ReadableStream({
    async start(controller) {
      const words = responseText.split(" ")
      for (const word of words) {
        controller.enqueue(
          new TextEncoder().encode(
            `data: {"type":"text-delta","textDelta":"${word.replace(/"/g, '\\"')} "}\n\n`
          )
        )
        await new Promise((resolve) => setTimeout(resolve, 50))
      }
      controller.enqueue(new TextEncoder().encode('data: {"type":"finish"}\n\n'))
      controller.close()
    },
  })

  return new Response(stream, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "X-Model-Used": selectedModel,
      "X-Model-Type": "simulated",
      "X-Knowledge-Enabled": knowledgeEnabled.toString(),
      "X-Learning-Mode": learningMode.toString(),
      "X-Random-Mode": useRandom.toString(),
    },
  })
}

// ImplementaÃ§Ã£o principal da API
export async function POST(req: Request) {
  try {
    const {
      messages,
      model = "llama3",
      personality = "friendly",
      knowledgeEnabled = false,
      learningMode = false,
      useRandom = false,
      files = [],
    } = await req.json()

    console.log(`ðŸ¤– Processando mensagem com modelo: ${model}`)

    // Construir prompt do sistema
    const systemPrompt = buildSystemPrompt(personality, knowledgeEnabled, learningMode, messages)

    // Verificar se Ã© um modelo local
    const isLocalModel = LOCAL_MODELS.includes(model)

    if (isLocalModel) {
      console.log(`ðŸ–¥ï¸ Tentando usar modelo local: ${model}`)
      
      // Verificar se Ollama estÃ¡ disponÃ­vel
      const ollamaStatus = await checkOllamaAvailability()
      
      if (ollamaStatus.available && ollamaStatus.models.includes(model)) {
        console.log(`âœ… Ollama disponÃ­vel com modelo ${model}`)
        try {
          return await generateOllamaResponse(messages, model, systemPrompt)
        } catch (error) {
          console.error(`âŒ Erro no Ollama com modelo ${model}:`, error)
          // Continua para fallback
        }
      } else {
        console.log(`âš ï¸ Ollama indisponÃ­vel ou modelo ${model} nÃ£o encontrado`)
        console.log(`Modelos disponÃ­veis:`, ollamaStatus.models)
        // Continua para fallback
      }
    }

    // Verificar se temos chaves de API para modelos de nuvem
    const hasOpenAI = !!process.env.OPENAI_API_KEY
    const hasAnthropic = !!process.env.ANTHROPIC_API_KEY

    // Se nÃ£o Ã© modelo local e temos chaves de API, usar modelo de nuvem
    if (!isLocalModel && (hasOpenAI || hasAnthropic)) {
      console.log(`â˜ï¸ Usando modelo de nuvem: ${model}`)
      
      try {
        let result
        
        switch (model) {
          case "claude-3-7-sonnet":
            if (hasAnthropic) {
              // Implementar Anthropic quando disponÃ­vel
              result = await streamText({
                model: openai("gpt-4o"), // Fallback temporÃ¡rio
                messages,
                system: systemPrompt,
              })
            } else {
              throw new Error("Chave da Anthropic nÃ£o configurada")
            }
            break
          case "gemini-pro":
            // Implementar Google AI quando disponÃ­vel
            result = await streamText({
              model: openai("gpt-4o"), // Fallback temporÃ¡rio
              messages,
              system: systemPrompt,
            })
            break
          case "gpt-4o":
          case "o3-mini":
          default:
            if (!hasOpenAI) {
              throw new Error("Chave da OpenAI nÃ£o configurada")
            }
            result = await streamText({
              model: openai(model === "o3-mini" ? "gpt-4o-mini" : "gpt-4o"),
              messages,
              system: systemPrompt,
            })
        }

        return result.toDataStreamResponse({
          headers: {
            "X-Model-Used": model,
            "X-Model-Type": "cloud",
            "X-Knowledge-Enabled": knowledgeEnabled.toString(),
            "X-Learning-Mode": learningMode.toString(),
            "X-Random-Mode": useRandom.toString(),
          },
        })
      } catch (error) {
        console.error(`âŒ Erro com modelo de nuvem ${model}:`, error)
        // Continua para modo demonstraÃ§Ã£o
      }
    }

    // Fallback para modo demonstraÃ§Ã£o
    console.log(`ðŸŽ­ Usando modo demonstraÃ§Ã£o para modelo: ${model}`)
    return await generateSimulatedResponse(
      messages,
      model,
      personality,
      knowledgeEnabled,
      learningMode,
      useRandom
    )

  } catch (error: any) {
    console.error("âŒ Erro geral na API:", error)

    // Resposta de erro amigÃ¡vel
    const errorStream = new ReadableStream({
      async start(controller) {
        const errorText = `Ops! ðŸ˜… Ocorreu um erro inesperado.\n\nðŸ”§ **PossÃ­veis soluÃ§Ãµes:**\n\n1. **Para IA Local (Ollama):**\n   â€¢ Instale: https://ollama.ai\n   â€¢ Execute: \`ollama serve\`\n   â€¢ Baixe modelo: \`ollama pull llama3\`\n\n2. **Para IA na Nuvem:**\n   â€¢ Configure OPENAI_API_KEY no .env.local\n   â€¢ Reinicie o servidor\n\n3. **Modo DemonstraÃ§Ã£o:**\n   â€¢ Funciona sem configuraÃ§Ã£o\n   â€¢ Respostas simuladas\n\nTente novamente ou verifique as configuraÃ§Ãµes! ðŸš€`

        const words = errorText.split(" ")
        for (const word of words) {
          controller.enqueue(
            new TextEncoder().encode(
              `data: {"type":"text-delta","textDelta":"${word.replace(/"/g, '\\"')} "}\n\n`
            )
          )
          await new Promise((resolve) => setTimeout(resolve, 30))
        }
        controller.enqueue(new TextEncoder().encode('data: {"type":"finish"}\n\n'))
        controller.close()
      },
    })

    return new Response(errorStream, {
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "X-Model-Used": "error-handler",
        "X-Model-Type": "fallback",
        "X-Error": error.message,
      },
    })
  }
}