import { openai } from "@ai-sdk/openai";
import { createOllama } from "ollama-ai-provider";
import { streamText } from "ai";

// ConfiguraÃ§Ã£o do Ollama
const OLLAMA_BASE_URL = "http://localhost:11434";

// Inicializa o provider do Ollama
const ollama = createOllama({
  baseURL: OLLAMA_BASE_URL,
});

const LOCAL_MODELS = {
  ollama: [
    "mistral:7b-instruct-q4_K_M",
    "llama3",
    "phi3",
    "gemma:2b",
    "llama3:8b",
    "llama3:70b",
    "mistral:latest",
    "codellama:latest",
    "neural-chat",
    "dolphin-mistral",
  ],
  webllm: [
    "Llama-2-7b-chat-hf-q4f32_1",
    "RedPajama-INCITE-Chat-3B-v1-q4f32_0",
    "Mistral-7B-Instruct-v0.1-q4f16_1",
  ],
};

// FunÃ§Ãµes auxiliares
function hasAPIKeys() {
  return !!(process.env.OPENAI_API_KEY || process.env.ANTHROPIC_API_KEY);
}

function isPreviewEnvironment() {
  return (
    process.env.VERCEL_URL?.includes("v0.dev") ||
    process.env.VERCEL_URL?.includes("vercel.app")
  );
}

// FunÃ§Ã£o para verificar se o Ollama estÃ¡ disponÃ­vel
async function checkOllamaAvailability() {
  try {
    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {
      method: "GET",
      headers: { "Content-Type": "application/json" },
    });
    
    if (!response.ok) return { available: false, models: [] };
    
    const data = await response.json();
    return {
      available: true,
      models: data.models?.map((model: any) => model.name) || [],
    };
  } catch (error) {
    console.error("Erro ao verificar Ollama:", error);
    return { available: false, models: [] };
  }
}

// FunÃ§Ã£o para gerar resposta com Ollama
async function generateOllamaResponse(messages: any[], model: string, systemPrompt: string) {
  try {
    // Verifica se o modelo estÃ¡ disponÃ­vel
    const ollamaStatus = await checkOllamaAvailability();
    if (!ollamaStatus.available || !ollamaStatus.models.includes(model)) {
      throw new Error(`Modelo ${model} nÃ£o disponÃ­vel no Ollama`);
    }

    // Usa o provider do Ollama
    const result = await streamText({
      model: ollama(model),
      messages: [
        { role: "system", content: systemPrompt },
        ...messages,
      ],
    });

    return result.toDataStreamResponse();
  } catch (error) {
    console.error("Erro ao gerar resposta com Ollama:", error);
    throw error;
  }
}

function searchKnowledgeBase(query: string) {
  const mockKnowledge = [
    {
      title: "InformaÃ§Ãµes sobre IA",
      content:
        "InteligÃªncia Artificial Ã© um campo da ciÃªncia da computaÃ§Ã£o que se concentra na criaÃ§Ã£o de sistemas capazes de realizar tarefas que normalmente requerem inteligÃªncia humana.",
      relevance: 0.9,
    },
    {
      title: "Aprendizado de MÃ¡quina",
      content:
        "Machine Learning Ã© um subcampo da IA que permite aos computadores aprender e melhorar automaticamente atravÃ©s da experiÃªncia sem serem explicitamente programados.",
      relevance: 0.8,
    },
  ];

  return mockKnowledge
    .filter(
      (item) =>
        item.content.toLowerCase().includes(query.toLowerCase()) ||
        item.title.toLowerCase().includes(query.toLowerCase())
    )
    .slice(0, 3);
}

// ImplementaÃ§Ã£o principal
export async function POST(req: Request) {
  try {
    const {
      messages,
      model,
      personality = "friendly",
      knowledgeEnabled = false,
      learningMode = false,
      useRandom = false,
    } = await req.json();

    // ConstrÃ³i o prompt do sistema
    const systemPrompt = buildSystemPrompt(personality, knowledgeEnabled, learningMode, messages);

    // Verifica se Ã© um modelo Ollama
    const isLocalModel =
      LOCAL_MODELS.ollama.includes(model) ||
      LOCAL_MODELS.webllm.includes(model);

    // Se for modelo local (Ollama), tenta usar primeiro
    if (isLocalModel) {
      console.log(`ðŸ–¥ï¸ Usando modelo local: ${model}`);
      try {
        return await generateOllamaResponse(messages, model, systemPrompt);
      } catch (error) {
        console.error("Erro no Ollama, usando fallback:", error);
        // Continua para o fallback
      }
    }

    // Fallback para simulaÃ§Ã£o se em preview ou sem chaves
    if (isPreviewEnvironment() || !hasAPIKeys()) {
      console.log("ðŸŽ­ Modo demonstraÃ§Ã£o ativo");
      return simulatedResponse(
        messages,
        model,
        personality,
        knowledgeEnabled,
        learningMode,
        useRandom
      );
    }

    // Fallback para modelos de nuvem com API key
    console.log(`â˜ï¸ Usando modelo de nuvem: ${model}`);
    return cloudResponse(
      messages,
      model,
      personality,
      knowledgeEnabled,
      learningMode,
      useRandom
    );
  } catch (error) {
    console.error("Erro na API:", error);
    const requestData = await req.json().catch(() => ({}));
    return errorResponse(error, requestData);
  }
}

// FunÃ§Ãµes auxiliares de resposta
function buildSystemPrompt(
  personality: string,
  knowledgeEnabled: boolean,
  learningMode: boolean,
  messages: any[]
) {
  const personalityPrompts = {
    friendly: "VocÃª Ã© NOVA, uma assistente virtual brasileira amigÃ¡vel, calorosa e descontraÃ­da. Use emojis e linguagem casual.",
    professional: "VocÃª Ã© NOVA, uma assistente virtual brasileira profissional, formal e precisa. Foque em eficiÃªncia.",
    creative: "VocÃª Ã© NOVA, uma assistente virtual brasileira criativa, imaginativa e inspiradora. Gere ideias inovadoras.",
    analytical: "VocÃª Ã© NOVA, uma assistente virtual brasileira analÃ­tica, lÃ³gica e detalhada. Base suas respostas em dados.",
    empathetic: "VocÃª Ã© NOVA, uma assistente virtual brasileira empÃ¡tica, compreensiva e cuidadosa. Foque no bem-estar."
  };

  let prompt = personalityPrompts[personality as keyof typeof personalityPrompts] || personalityPrompts.friendly;

  if (knowledgeEnabled) {
    const lastMessage = messages[messages.length - 1]?.content || "";
    const knowledge = searchKnowledgeBase(lastMessage);
    if (knowledge.length > 0) {
      prompt += `\n\nBase de conhecimento:\n${knowledge
        .map((k) => `- ${k.title}: ${k.content}`)
        .join("\n")}`;
    }
  }

  if (learningMode) prompt += "\n\nVocÃª estÃ¡ em modo de aprendizado contÃ­nuo.";

  return prompt;
}

function buildHeaders(
  model: string,
  type: string,
  knowledgeEnabled: boolean,
  learningMode: boolean,
  useRandom: boolean = false
) {
  return {
    "Content-Type": "text/plain; charset=utf-8",
    "X-Model-Used": model,
    "X-Model-Type": type,
    "X-Knowledge-Enabled": knowledgeEnabled.toString(),
    "X-Learning-Mode": learningMode.toString(),
    "X-Random-Mode": useRandom.toString(),
  };
}

async function simulatedResponse(
  messages: any[],
  model: string,
  personality: string,
  knowledgeEnabled: boolean,
  learningMode: boolean,
  useRandom: boolean = false
) {
  // Se modo aleatÃ³rio estiver ativo, escolhe um modelo aleatÃ³rio
  let selectedModel = model;
  if (useRandom) {
    const allModels = [
      ...LOCAL_MODELS.ollama,
      "gpt-4o",
      "claude-3-7-sonnet",
      "gemini-pro",
      "grok-beta",
      "deepseek-chat",
    ];
    selectedModel = allModels[Math.floor(Math.random() * allModels.length)];
  }

  // Gera resposta baseada na personalidade e contexto
  const lastMessage = messages[messages.length - 1]?.content || "";
  let responseText = "";

  // Respostas contextuais baseadas na personalidade
  switch (personality) {
    case "friendly":
      responseText = `Oi! ðŸ˜Š Sou NOVA em modo demonstraÃ§Ã£o usando ${selectedModel}. ${lastMessage ? `Sobre "${lastMessage}", ` : ""}estou aqui para ajudar de forma amigÃ¡vel e descontraÃ­da! Como posso te ajudar hoje?`;
      break;
    case "professional":
      responseText = `OlÃ¡. Sou NOVA em modo demonstraÃ§Ã£o utilizando ${selectedModel}. ${lastMessage ? `Referente Ã  sua consulta "${lastMessage}", ` : ""}estou preparada para fornecer assistÃªncia profissional e eficiente.`;
      break;
    case "creative":
      responseText = `OlÃ¡! âœ¨ Sou NOVA em modo criativo usando ${selectedModel}. ${lastMessage ? `Sua ideia sobre "${lastMessage}" Ã© interessante! ` : ""}Vamos explorar possibilidades criativas juntos!`;
      break;
    case "analytical":
      responseText = `SaudaÃ§Ãµes. NOVA em modo analÃ­tico com ${selectedModel}. ${lastMessage ? `Analisando "${lastMessage}": ` : ""}Estou pronta para fornecer anÃ¡lises detalhadas e baseadas em dados.`;
      break;
    case "empathetic":
      responseText = `OlÃ¡, querido! ðŸ’™ Sou NOVA em modo empÃ¡tico usando ${selectedModel}. ${lastMessage ? `Entendo sua questÃ£o sobre "${lastMessage}" e ` : ""}estou aqui para ouvir e apoiar vocÃª.`;
      break;
    default:
      responseText = `OlÃ¡! Sou NOVA usando ${selectedModel} em modo demonstraÃ§Ã£o. Como posso ajudar?`;
  }

  if (knowledgeEnabled) {
    responseText += "\n\nðŸ§  Sistema de conhecimento ativo - posso acessar informaÃ§Ãµes da base de dados.";
  }

  if (learningMode) {
    responseText += "\n\nðŸ“š Modo aprendizado ativo - estou aprendendo continuamente com nossas interaÃ§Ãµes.";
  }

  const stream = new ReadableStream({
    async start(controller) {
      const words = responseText.split(" ");
      for (const word of words) {
        controller.enqueue(
          new TextEncoder().encode(
            `data: {"type":"text-delta","textDelta":"${word.replace(
              /"/g,
              '\\"'
            )} "}\n\n`
          )
        );
        await new Promise((resolve) => setTimeout(resolve, 50));
      }
      controller.enqueue(
        new TextEncoder().encode('data: {"type":"finish"}\n\n')
      );
      controller.close();
    },
  });

  return new Response(stream, {
    headers: buildHeaders(
      selectedModel,
      "simulated",
      knowledgeEnabled,
      learningMode,
      useRandom
    ),
  });
}

async function cloudResponse(
  messages: any[],
  model: string,
  personality: string,
  knowledgeEnabled: boolean,
  learningMode: boolean,
  useRandom: boolean = false
) {
  const systemPrompt = buildSystemPrompt(
    personality,
    knowledgeEnabled,
    learningMode,
    messages
  );
  
  // Seleciona o modelo apropriado
  let selectedModel = model;
  if (useRandom) {
    const cloudModels = ["gpt-4o", "claude-3-7-sonnet", "gemini-pro"];
    selectedModel = cloudModels[Math.floor(Math.random() * cloudModels.length)];
  }

  let result;
  try {
    // Tenta usar o modelo especÃ­fico
    switch (selectedModel) {
      case "claude-3-7-sonnet":
        // Implementar Anthropic quando disponÃ­vel
        result = await streamText({
          model: openai("gpt-4o"), // Fallback
          messages,
          system: systemPrompt,
        });
        break;
      case "gemini-pro":
        // Implementar Google AI quando disponÃ­vel
        result = await streamText({
          model: openai("gpt-4o"), // Fallback
          messages,
          system: systemPrompt,
        });
        break;
      default:
        result = await streamText({
          model: openai(selectedModel === "gpt-4o" ? "gpt-4o" : "gpt-4o"),
          messages,
          system: systemPrompt,
        });
    }
  } catch (error) {
    console.error(`Erro com modelo ${selectedModel}, usando GPT-4o:`, error);
    result = await streamText({
      model: openai("gpt-4o"),
      messages,
      system: systemPrompt,
    });
  }

  return result.toDataStreamResponse({
    headers: buildHeaders(selectedModel, "cloud", knowledgeEnabled, learningMode, useRandom),
  });
}

function errorResponse(error: Error, requestData: any) {
  const {
    model = "mistral:7b-instruct-q4_K_M",
    personality = "friendly",
    knowledgeEnabled = false,
    learningMode = false,
    useRandom = false,
  } = requestData;

  let errorText = "";
  
  if (error.message.includes("Ollama") || error.message.includes("localhost:11434")) {
    errorText = `ðŸ”§ Ollama nÃ£o estÃ¡ disponÃ­vel.\n\nPara usar modelos locais:\n1. Instale o Ollama (https://ollama.ai)\n2. Execute: ollama serve\n3. Baixe um modelo: ollama pull ${model}\n\nEnquanto isso, posso funcionar em modo demonstraÃ§Ã£o! ðŸ˜Š`;
  } else {
    errorText = `Ops! ðŸ˜… Ocorreu um erro:\n\n${error.message}\n\nEstou em modo de recuperaÃ§Ã£o. Tente novamente ou verifique se o Ollama estÃ¡ rodando.`;
  }

  const stream = new ReadableStream({
    async start(controller) {
      const words = errorText.split(" ");
      for (const word of words) {
        controller.enqueue(
          new TextEncoder().encode(
            `data: {"type":"text-delta","textDelta":"${word.replace(
              /"/g,
              '\\"'
            )} "}\n\n`
          )
        );
        await new Promise((resolve) => setTimeout(resolve, 50));
      }
      controller.enqueue(
        new TextEncoder().encode('data: {"type":"finish"}\n\n')
      );
      controller.close();
    },
  });

  return new Response(stream, {
    headers: buildHeaders(
      model,
      "error-fallback",
      knowledgeEnabled,
      learningMode,
      useRandom
    ),
  });
}
